**1、权重是怎么映射的

这个项目里，权重到器件其实是三步链路：
`W(有符号权重) -> G(正值电导) -> R(电阻=1/G)`。

下面按你当前仓库实现，逐层细讲。

**1. 先把“正负权重”拆成两条器件支路**
公式是：
```text
W_pos = max(W, 0)
W_neg = max(-W, 0)
W = W_pos - W_neg
```
含义：
1. 正权重只写到正支路列（pos列）。
2. 负权重不写“负电阻”，而是写到负支路列（neg列）。
3. 0 权重会变成 pos=0、neg=0。

这就是差分存储的核心。

**2. 为什么要用 shared_max（统一标尺）**


公式：
```text
shared_max = max(abs(W))
n_pos = W_pos / shared_max
n_neg = W_neg / shared_max
```
这样做的原因：
1. 正负支路用同一量化刻度，避免“正支路量化细、负支路量化粗”的失真。
2. 差分后更接近原始权重符号和幅值关系。

**3. 4-bit 量化不是线性，而是贴器件离散电导级**

你文档记录的实测参数是：
`R_off=4.18e11Ω, R_on=2.34e8Ω, on/off≈1786`（[SNNSoC工程主文档.md:1288](d:\SoC Design\SoC Design\SNNSoC工程主文档.md:1288)）。

换成电导：
```text
g_min = 1/R_off ≈ 2.392e-12 S
g_max = 1/R_on  ≈ 4.274e-9 S
```

16级电导（k=0..15）可写成：
```text
G_k = g_min * (g_max/g_min)^(k/15)
R_k = 1/G_k
```
所以每升一级，R大约除以 1.647（对数间隔）。

**4. 具体怎么把 n_pos/n_neg 映射到级别**
代码在 [snn_engine.py:273](d:\SoC Design\SoC Design\项目相关文件\器件对齐\Python建模\snn_engine.py:273)。

核心是“最近邻”：
```text
k = argmin | n - (G_k / g_max) |
```
然后得到目标电导 G，再转成电阻 R=1/G。  
并且会 clamp 到 `[g_min, g_max]`（[snn_engine.py:295](d:\SoC Design\SoC Design\项目相关文件\器件对齐\Python建模\snn_engine.py:295)）。

注意一个非常关键的工程事实：
1. 0 权重不会变成 G=0（开路），而是落到最小电导 `g_min`（即最大电阻 `R_off`）。
2. 所以阵列天然有“漏导通底噪”，这是器件真实约束，不是 bug。

**5. 导出给器件同学的 CSV 是什么**
代码在 [export_weight_map.py:127](d:\SoC Design\SoC Design\项目相关文件\器件对齐\Python建模\export_weight_map.py:127)。

每行字段：
`row,col_pos,col_neg,level_pos,level_neg,G_pos,G_neg`

含义：
1. `row`：输入维索引 i（0..63）。
2. `col_pos/col_neg`：该输出类别 j 的正负列。
3. `level_pos/level_neg`：电导级编号（0..15）。
4. `G_pos/G_neg`：目标电导值（S）。

默认导出列号是：
`col_pos=2*j, col_neg=2*j+1`（[export_weight_map.py:130](d:\SoC Design\SoC Design\项目相关文件\器件对齐\Python建模\export_weight_map.py:130)）。

你接下来直接重新导出 weight map 即可（旧 CSV 建议作废）：

`python export_weight_map.py --method proj_sup_64 --weight-bits 4`

**6. 按你参数给一个完整数值例子（负权重）**
假设某个权重 `W=-0.30`，该层 `shared_max=0.80`。

1. 差分拆分：
`W_pos=0`, `W_neg=0.30`
2. 归一化：
`n_pos=0`, `n_neg=0.375`
3. 最近邻量化：
`n_neg=0.375` 最接近级别大约 `k=13`（归一化级约 0.368）
4. 得到器件值：
`G_neg≈1.575e-9 S`, `R_neg≈6.35e8 Ω`  
`G_pos≈g_min≈2.392e-12 S`, `R_pos≈4.18e11 Ω`
5. 物理含义：
负支路明显更导通，正支路接近关断。
6. 读出时（Scheme B）做差分（RTL逻辑口径）：
`diff = pos - neg`，因此贡献是负号。

**7. 再给两个边界例子**
1. `W=+0.52`（shared_max=0.8）  
`n=0.65 -> k≈14 -> R_pos≈3.85e8Ω, R_neg≈4.18e11Ω`，贡献为正。
2. `W=0`  
`pos/neg 都到 k=0（R_off）`，理想差分接近 0，但失配和噪声下会有残差。

**8. 你现在最容易混淆的一个点（很重要）**
1. Python 导出列号默认是“交错对列”（2j,2j+1）。
2. RTL 文档逻辑通道是“0~9 正、10~19 负”（见 [SNNSoC工程主文档.md:1441](d:\SoC Design\SoC Design\SNNSoC工程主文档.md:1441)）。

这两者可以兼容，但前提是你和模拟侧明确 `bl_sel -> 物理列` 的重排关系（MUX/LUT）。这一步必须写死在 handoff 文档里，避免后续“权重写对了但读错列”。

2、建模

你这个理解基本对，给你一个“严格版”：

1. 先正常训练 ANN（浮点权重）。
2. 如果不开 QAT：训练完直接量化一次，再去做 SNN 推理，这叫 PTQ（先训后量化）。
3. 如果开 QAT：在训练后再做一段“量化感知微调”。
4. QAT 每个 step 都是：先把浮点权重临时量化成离散值做前向和 loss；反向时梯度通过 STE 回到浮点主权重；优化器更新的是浮点主权重。
5. 下一个 step 再把更新后的浮点权重量化，循环这个过程。

你那句话可以改成更准确的版本：
“前向用离散权重算 loss，反向不直接更新离散权重，而是用 STE 把梯度传回浮点主权重去更新。”

为什么要这样做：
1. 离散量化本身不可导（round/最近级别选择会让梯度断掉）。
2. 直接对离散权重做梯度下降基本不可行。
3. STE 是工程近似：前向像硬件，反向还能学。

你问的“那直接 ANN 学 W 然后替换不就行了？”
1. 行，这就是 PTQ，很多项目先这么做。
2. 只有当 PTQ 后精度掉得明显，才需要 QAT 去“把权重往量化友好方向再推一段”。

你们建模里“非理想特性”主要体现在这些层面：

1. 权重量化非理想  
把连续权重映射到有限电导级（例如 4-bit 16 级），天然有量化误差。

2. 差分映射非理想  
正负权重拆成正支路/负支路电导，再做差分，拆分本身会引入额外误差源。

3. D2D/C2C 随机波动  
D2D 表示整片整体偏移，C2C 表示单元到单元随机偏差，都会乘到电导上。

4. 读噪声  
读出信号会叠加噪声，影响每次 MAC/ADC 前的模拟值。

5. IR-drop  
阵列互连电阻导致有效电压衰减，输入越强或阵列负载越大时误差越明显。

6. 漂移（drift）  
电导随时间/读写发生漂移，不再等于最初写入值。

7. ADC 量化与饱和  
模拟值进 ADC 后再被有限位宽量化并可能裁剪，进一步损失精度。

8. 脉冲判决误差  
阈值、timesteps、reset 模式会影响 spike 统计结果，这是数字侧推理误差来源。

再给你一句落地判断：
1. 你现在可以把流程理解成“ANN 负责学权重，SNN/CIM 负责在硬件约束下执行这个权重”。
2. QAT 只是让 ANN 在训练时提前看到这些硬件约束，减少部署后掉点。


3、
## 1. 先记术语（最简版）

1. logits  
    模型输出的原始分数（还不是概率），例如 [[3.2, -3.2]](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/24201/.vscode/extensions/openai.chatgpt-0.4.78-win32-x64/webview/#)。
    
2. softmax  
    把 logits 变成概率：
    

pi=esi∑jesjpi​=∑j​esj​esi​​

3. 交叉熵（Cross Entropy）  
    分类损失。真实类别是 yy 时：

L=−log⁡(py)L=−log(py​)

真实类概率越大，loss 越小。

4. W_cls（或临时分类头 WauxWaux​）  
    把降维后的特征 zz 映射成类别分数（logits）的矩阵。  
    如果 z∈R4z∈R4，二分类时 Wcls∈R2×4Wcls​∈R2×4；10分类时就是 10×410×4（真实项目是 10×6410×64）。

---

## 2. 任务设定：4x4 -> 2x2（即 16维 -> 4维）

输入图：

X=[9810891010890198]X=​9810​8901​1189​0098​​

展平为 x∈R16x∈R16。

目标：学一个线性投影，把 xx 变成 z∈R4z∈R4，再重排成 2×22×2。

---

## 3. 有监督线性投影降维（Supervised Projection）

### 3.1 模型形式

z=Px+bp,logits=Wauxz+bcz=Px+bp​,logits=Waux​z+bc​

其中 P∈R4×16P∈R4×16 是要学的“降维矩阵”。

### 3.2 数值例子（前向）

为了手算，先用“4个象限均值”作为 PP 的当前值（训练中它会被更新）：

- z1=z1​= 左上2x2均值 =(9+8+8+9)/4=8.5=(9+8+8+9)/4=8.5
- z2=z2​= 右上2x2均值 =(1+0+1+0)/4=0.5=(1+0+1+0)/4=0.5
- z3=z3​= 左下2x2均值 =(1+0+0+1)/4=0.5=(1+0+0+1)/4=0.5
- z4=z4​= 右下2x2均值 =(8+9+9+8)/4=8.5=(8+9+9+8)/4=8.5

所以

z=[8.5, 0.5, 0.5, 8.5]z=[8.5,0.5,0.5,8.5]

重排成 2×22×2：

[8.50.50.58.5][8.50.5​0.58.5​]

设二分类临时头：

Waux=[0.2−0.2−0.20.2−0.20.20.2−0.2]Waux​=[0.2−0.2​−0.20.2​−0.20.2​0.2−0.2​]

得到 logits：

- s0=0.2(8.5)−0.2(0.5)−0.2(0.5)+0.2(8.5)=3.2s0​=0.2(8.5)−0.2(0.5)−0.2(0.5)+0.2(8.5)=3.2
- s1=−3.2s1​=−3.2

softmax：

p0≈0.99834,p1≈0.00166p0​≈0.99834,p1​≈0.00166

若真实标签是类0：

L=−log⁡(0.99834)≈0.00166L=−log(0.99834)≈0.00166

### 3.3 训练到底在做什么

每个 batch 都做：

1. 前向算 zz、logits、softmax、CE
2. 反向传播
3. 同时更新 PP 和 WauxWaux​

意义：  
PP 会被训练成“对分类最有用的4维特征提取器”，不是单纯压缩。

> 真实项目是10分类，原理完全一样，只是 WauxWaux​ 从 2×42×4 换成 10×410×4（或 10×6410×64）。

---

## 4. 无监督降维（以 PCA 为例）

无监督：不使用标签，只看数据本身。

### 4.1 PCA流程

1. 收集很多 x∈R16x∈R16（无标签）
2. 去均值
3. 算协方差，取前4个主成分 v1∼v4v1​∼v4​
4. 投影：

zi=vi⊤(x−μ), i=1..4zi​=vi⊤​(x−μ), i=1..4

### 4.2 数值例子（手算版）

假设从数据中得到4个主方向（示意）：

- v1v1​：整体亮度方向（全1）
- v2v2​：左右对比方向（左+右-）
- v3v3​：上下对比方向（上+下-）
- v4v4​：棋盘对比方向（交错+/-）

为方便手算，取系数 1/161/16。对上面那张 XX：

1. 总和 =72=72

z1=72/16=4.5z1​=72/16=4.5

2. 左右和都为36

z2=(36−36)/16=0z2​=(36−36)/16=0

3. 上下和都为36

z3=(36−36)/16=0z3​=(36−36)/16=0

4. 棋盘加权和 =2=2

z4=2/16=0.125z4​=2/16=0.125

所以无监督特征：

z=[4.5, 0, 0, 0.125]z=[4.5,0,0,0.125]

重排成 2×22×2：

[4.5000.125][4.50​00.125​]

注意：这4维是“方差最大方向”的信息，不保证最利于分类。  
如果要分类，还得再训练一个分类器。

---

## 5. 两种方式一句话对比

1. 有监督投影：  
    “为了分类准”去学降维矩阵 PP。
    
2. 无监督投影（PCA）：  
    “为了保留数据主信息”去学投影，不看标签。
    

---

## 6. 你项目里的实用理解

1. 你最终需要的是 64维特征给后级 FC/SNN。
2. 如果目标是准确率优先，通常有监督投影更强。
3. 无监督投影更通用、解释性更强，但分类任务上未必最佳。

train / val / test 的作用：

1. train  
    用来更新权重（反向传播只在这里发生）。
    
2. val  
    用来调参数和选方案（阈值比、方法、scheme、ADC位宽、权重位宽、时间步等）。  
    不参与权重更新，只做“选配置”。
    
3. test  
    只在最后做最终报告，尽量不参与调参。  
    目的是避免“拿测试集调参”导致结果虚高


**val 用来“比较候选配置谁更好”，然后只把最好配置带去 test 做一次最终评估。**

你这个项目里具体是这样用的：

1. 阈值比标定（如果开启）  
    在 val 上遍历多个 threshold_ratio 候选，选 val 准确率最高的 ratio。
    
2. 下采样/投影方法选择  
    在 val 上比较 proj_sup_64 / proj_pca_64 / 各种8x8方法，看谁的 SNN 准确率更高。
    
3. 全量组合扫参（核心）  
    在 val 上遍历组合：  
    method × scheme × ADC_BITS × WEIGHT_BITS × TIMESTEPS  
    每个组合都会得到一个 val 准确率。
    
4. 选 best-case 和 recommendation
    
    - best-case：val 准确率最高的组合
    - recommendation：在“接近 best-case（允许小幅精度差）”里选成本更低的组合（更少位宽/更少时间步等）
5. 最后才上 test  
    把上一步选出的配置在 test 上跑一次，作为最终报告。  
    这样 test 不参与调参，结果更可信。